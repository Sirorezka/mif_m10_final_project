{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from lxml import html, etree\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "PROXIES = {\n",
    " ## 'http': 'http://VPN4726:@ar.finevpn.org',\n",
    " ## 'https': 'https://VPN4726:@ar.finevpn.org',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## http://docs.python-guide.org/en/latest/scenarios/scrape/\n",
    "##\n",
    "\n",
    "class news_forbes_parser():\n",
    "    def __init__(self):\n",
    "        self.link = 'https://www.forbes.com/search/?q=bitcoin'\n",
    "        self.save_dir = \"news_forbes/\"\n",
    "    \n",
    "    def read_article_text(self,article_href, verbose=False):\n",
    "        \n",
    "        text = \"\"\n",
    "        has_next = True  ## if need to read next page\n",
    "        cur_page = 0\n",
    "        \n",
    "        while has_next:\n",
    "            ## read current page text\n",
    "            cur_page = cur_page + 1\n",
    "            has_next = False\n",
    "            try:\n",
    "                article_page = requests.get(article_href + str(cur_page), proxies=PROXIES)\n",
    "                article_html = html.fromstring(article_page.content)\n",
    "            except:\n",
    "                print (\"can't read page \",article_href)\n",
    "                \n",
    "            try:\n",
    "                article_text = article_html.xpath('//article-body-container[@class=\"article-body fs-article fs-responsive-text\"]')[0]\n",
    "                article_text = etree.tostring(article_text).decode(\"utf-8\") \n",
    "            except:\n",
    "                print (\"error reading text from page: \",article_href)\n",
    "                article_text = \"\"           \n",
    "            text = text +\" \" + article_text\n",
    "            \n",
    "            ## has next page\n",
    "            try:\n",
    "                pages_nav = article_html.xpath('//footer[@class=\"article-footer\"]')[0]\n",
    "                pages_nav = article_html.xpath('.//pagination-nav')[0]\n",
    "                pages_nav = article_html.xpath('.//a[@class=\"next\"]')\n",
    "                if len(pages_nav)>0: has_next = True\n",
    "            except:\n",
    "                ##print (\"error reading page_num \",article_href)\n",
    "                pass\n",
    "\n",
    "        text = re.sub(\"(\\n|\\t)\",\" \",text)\n",
    "        ## text = re.sub(\"&lt;\",\"<\",text)\n",
    "        ## text = re.sub(\"&gt;\",\">\",text)\n",
    "        ## stext = re.sub(\"&quot;\",'\"',text)\n",
    "        text = re.sub(\"<tweet-quotes.*?</tweet-quotes>\",\"\",text)    ## examples with twitter\n",
    "        text = re.sub(\"<fbs-accordion.*?</fbs-accordion>\",\"\",text)  ## text under images\n",
    "        text = re.sub(\"<sig-file .*?</sig-file>\",\"\",text)  ## end of article\n",
    "        text = re.sub('<small class=\"article-photo-credit.*?</small>',\"\",text)\n",
    "        \n",
    "        \n",
    "        ##if verbose: print (text)\n",
    "        return text\n",
    "    \n",
    "    def clean_text (self,article_text):\n",
    "        article_text = re.sub(\"<script>.*?</script>\",\"\",article_text)\n",
    "        article_text = re.sub(\"<.*?>\",\"\",article_text)\n",
    "        article_text = re.sub(\"&#8217;\",\"'\",article_text)\n",
    "        article_text = re.sub(\"&amp;\",\"&\",article_text)\n",
    "        article_text = re.sub(\"&#[0-9]+;|\\n\",\" \",article_text)\n",
    "        article_text = re.sub(\" {1,}\",\" \",article_text)\n",
    "\n",
    "        return article_text\n",
    "\n",
    "    def parse_page(self,page_num, verbose = True, docs_per_page = 10):\n",
    "        \n",
    "        page_link = \"https://www.forbes.com/forbesapi/search/all.json\"\n",
    "        data_val = {\n",
    "        'limit':docs_per_page,\n",
    "        'query':'bitcoin',\n",
    "        'retrievedfields':'author,date,description,title,type,uri',\n",
    "        'sort':'date',\n",
    "        'start':(page_num-1)*docs_per_page}\n",
    "\n",
    "        r = requests.get(page_link, params = data_val, proxies = PROXIES)    \n",
    "\n",
    "        data_dump = json.loads(r.text)\n",
    "        if 'contentList' not in data_dump.keys():\n",
    "            print (\"error on requesting page \", page_num)\n",
    "\n",
    "        for i_article in data_dump['contentList']:\n",
    "\n",
    "            ## parsing date\n",
    "            ##\n",
    "            art_href =  i_article['uri']\n",
    "            \n",
    "            art_time = i_article['date']\n",
    "            art_time = str(datetime.datetime.utcfromtimestamp(art_time/1000).strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "            art_date = art_time[0:10]    \n",
    "\n",
    "            art_title = i_article['title']\n",
    "            art_title = bc_parser.clean_text(art_title)\n",
    "\n",
    "            ### description\n",
    "            try:\n",
    "                art_cont = i_article['description']\n",
    "                art_cont = bc_parser.clean_text(art_cont)\n",
    "            except:\n",
    "                print(\"Description not available\",art_href)\n",
    "                art_cont = \"\"\n",
    "            \n",
    "            ### author\n",
    "            try:\n",
    "                art_authour = i_article['author']\n",
    "            except:\n",
    "                print (\"author not available \", art_href)\n",
    "                art_authour = 'None'\n",
    "                \n",
    "            art_type = i_article['type']\n",
    "\n",
    "\n",
    "            if verbose: print (art_date,art_time,art_type,art_authour, art_title, art_cont, art_href)\n",
    "                \n",
    "            ## read article text\n",
    "            article_text = self.read_article_text(art_href,verbose=verbose)\n",
    "            article_text = self.clean_text(article_text)\n",
    "            \n",
    "            ## saving article\n",
    "            fn = re.sub(\"[^a-zA-Z0-9 ]\",\"\",art_title[0:20])\n",
    "            ## file_name = self.save_dir + art_date + \"_\" + 'p_' + '{num:03d}'.format(num=page_num) + \"_\" + fn + \".txt\"\n",
    "            file_name = self.save_dir + art_date + \"_\"  + fn + \".txt\"\n",
    "\n",
    "            if verbose: print ('FILE NAME: ',file_name)\n",
    "            f = open(file_name,'w', encoding = 'utf-8')\n",
    "            f.write(art_title + \"\\n\")\n",
    "            f.write(art_time + \"\\n\")\n",
    "            f.write(\"<uri>\" + art_href + \"</uri>\\n\")\n",
    "            f.write(\"<brief>\" + art_cont + \"</brief>\\n\")\n",
    "            f.write(\"<author>\" + art_authour + \"</author>\\n\")\n",
    "            f.write(\"<type>\" + art_type + \"</type>\\n\")\n",
    "            f.write(article_text)\n",
    "            f.close()\n",
    "            \n",
    "        print (\"page \" + str(page_num) + \" parsed\")\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "bc_parser = news_forbes_parser()\n",
    "\n",
    "for i in range(1,400):\n",
    "    bc_parser.parse_page(i,verbose=False, docs_per_page = 10)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "a_href = \"https://www.forbes.com/sites/rachelwolfson/2018/03/29/an-explanation-for-the-rise-of-stable-coins-as-a-low-volatility-cryptocurrency/\"\n",
    "a_text = bc_parser.read_article_text(a_href)    \n",
    "##bc_parser.clean_text(a_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
