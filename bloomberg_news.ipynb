{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from lxml import html, etree\n",
    "import time\n",
    "\n",
    "\n",
    "PROXIES = {\n",
    "  'http': 'http://VPN4726:24ZokIf2ut@ar.finevpn.org',\n",
    "  'https': 'https://VPN4726:24ZokIf2ut@ar.finevpn.org',\n",
    "}\n",
    "\n",
    "PROXIES ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1 parsed\n",
      "page 2 parsed\n",
      "page 3 parsed\n",
      "page 4 parsed\n",
      "page 5 parsed\n",
      "page 6 parsed\n",
      "page 7 parsed\n",
      "page 8 parsed\n",
      "page 9 parsed\n",
      "page 10 parsed\n"
     ]
    }
   ],
   "source": [
    "## http://docs.python-guide.org/en/latest/scenarios/scrape/\n",
    "##\n",
    "\n",
    "class news_bloomberg_parser():\n",
    "    def __init__(self):\n",
    "        ## self.link = 'https://www.bloomberg.com/search?query=bitcoin&endTime=2018-03-29T11:05:45.378Z&page='\n",
    "        self.link = 'https://www.bloomberg.com/search?query=bitcoin&endTime=2018-04-22T11:45:05.541Z&page='\n",
    "\n",
    "        self.save_dir = \"news_bloomberg/\"\n",
    "    \n",
    "    def read_article_text(self,article_href):\n",
    "        key_done = 0\n",
    "        while key_done <3:\n",
    "            try:\n",
    "                article_page = requests.get(article_href, proxies=PROXIES)\n",
    "                key_done = 3\n",
    "            except:\n",
    "                print ('error reading page ',article_href)\n",
    "                time.sleep(20)\n",
    "                key_done +=1\n",
    "                if key_done ==3:\n",
    "                    return \"\"\n",
    "        article_html = html.fromstring(article_page.content)\n",
    "        try:\n",
    "            article_text = article_html.xpath('//article/*/section[@class=\"main-column\"]')[0]\n",
    "        except:\n",
    "            print ('page not found ',article_href)\n",
    "            return \"\"\n",
    "        article_text = article_text.xpath('.//div[@class=\"body-copy fence-body\"]')[0]\n",
    "        text = etree.tostring(article_text).decode(\"utf-8\") \n",
    "        text = re.sub(\"<figure.*?</figure>\",\"\",text)\n",
    "        text = re.findall(\"<p>.*?</p>\",text)\n",
    "        text = \"\".join([x for x in text])\n",
    "        return text\n",
    "    \n",
    "    def clean_text (self,article_text):\n",
    "        article_text = re.sub(\"<.*?>\",\"\",article_text)\n",
    "        article_text = re.sub(\"&#8217;\",\"'\",article_text)\n",
    "        article_text = re.sub(\"&#[0-9]+;|\\n\",\" \",article_text)\n",
    "        article_text = re.sub(\" {1,}\",\" \",article_text)\n",
    "\n",
    "        return article_text\n",
    "\n",
    "    def parse_page(self,page_num, verbose = True):\n",
    "        page_link = self.link + str(page_num)\n",
    "        \n",
    "        if verbose: print (page_link)\n",
    "        \n",
    "        page = requests.get(page_link, proxies=PROXIES)\n",
    "        page_html = html.fromstring(page.content)\n",
    "        \n",
    "        news = page_html.xpath('//div[@class=\"search-result-items\"]')\n",
    "        ## print (news)\n",
    "        articles = news[0].xpath('.//div[@class=\"search-result-story__container\"]')\n",
    "        # print (articles)\n",
    "        ## for debugging\n",
    "        self.page = page\n",
    "        self.page_html = page_html\n",
    "        self.articles = articles \n",
    "        if verbose: print ('read page ' + str(page_num))\n",
    "        \n",
    "        for i_article in articles:\n",
    "            article_name = i_article.xpath('.//h1[@class=\"search-result-story__headline\"]/a')[0]\n",
    "            article_name = etree.tostring(article_name).decode(\"utf-8\") \n",
    "            article_name = self.clean_text(article_name)\n",
    "            if verbose: print (article_name)\n",
    "            \n",
    "            article_time = i_article.xpath('.//time[@class=\"published-at\"]/@datetime')[0]\n",
    "            article_date = article_time[0:10]\n",
    "            if verbose: print (article_time)\n",
    "\n",
    "            article_href = i_article.xpath('.//h1[@class=\"search-result-story__headline\"]/a/@href')[0]\n",
    "            if verbose: print (article_href)\n",
    "\n",
    "            article_content = i_article.xpath('.//div[@class=\"search-result-story__body\"]')[0]\n",
    "            article_content = etree.tostring(article_content).decode(\"utf-8\") \n",
    "            article_content = self.clean_text(article_content)\n",
    "            if verbose: print (article_content)\n",
    "            \n",
    "            if len(re.findall('video',article_href))>0:\n",
    "                article_text = \"\"\n",
    "                article_c_type = 'video'\n",
    "            elif len(re.findall('bloombergview',article_href))>0:\n",
    "                article_text = self.read_article_text(article_href)\n",
    "                article_c_type = 'view'\n",
    "            elif len(re.findall('news/audio',article_href))>0:\n",
    "                article_text = \"\"\n",
    "                article_c_type = 'audio'\n",
    "            elif len(re.findall('news/features',article_href))>0:\n",
    "                article_text = \"\"  ## will do this part later\n",
    "                article_c_type = 'feature'\n",
    "            elif len(re.findall('news/photo-essays',article_href))>0:\n",
    "                article_text = \"\"  ## will do this part later\n",
    "                article_c_type = 'photo-essay'\n",
    "            else:\n",
    "                article_text = self.read_article_text(article_href)\n",
    "                article_c_type = 'news'\n",
    "\n",
    "            ## cleaning article from special symbols\n",
    "            article_text = self.clean_text(article_text)\n",
    "                        \n",
    "            ## saving article\n",
    "            fn = re.sub(\"[^a-zA-Z0-9 ]\",\"\",article_name[0:20])\n",
    "            file_name = self.save_dir + article_date + \"_\" + 'p_' + '{num:03d}'.format(num=page_num) + \"_\" + fn + \".txt\"\n",
    "            if verbose: print (file_name)\n",
    "            f = open(file_name,'w', encoding = 'utf-8')\n",
    "            f.write(article_name + \"\\n\")\n",
    "            f.write(article_time + \"\\n\")\n",
    "            f.write(\"<brief>\" + article_content + \"</brief>\\n\")\n",
    "            f.write(\"<content>\" + article_c_type + \"</content>\\n\")\n",
    "            f.write(article_text)\n",
    "            f.close()\n",
    "        print (\"page \" + str(page_num) + \" parsed\")\n",
    "\n",
    "            \n",
    "bc_parser = news_bloomberg_parser()\n",
    "\n",
    "for i in range(1,12):\n",
    "    bc_parser.parse_page(i,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
